{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-05-07T20:27:50.010975300Z",
     "start_time": "2024-05-07T20:27:44.333708600Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-08 05:27:45.370260: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-08 05:27:45.370315: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-08 05:27:45.425417: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-05-08 05:27:45.543344: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-08 05:27:46.627071: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python:  sys.version_info(major=3, minor=9, micro=19, releaselevel='final', serial=0)\n",
      "sklearn version:  1.4.1.post1\n",
      "TF version:  2.15.0\n",
      "GPU installed:  True\n",
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-08 05:27:48.777344: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-08 05:27:48.951620: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-08 05:27:48.951669: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-08 05:27:48.954722: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-08 05:27:48.954766: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-08 05:27:48.954784: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-08 05:27:49.109351: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-08 05:27:49.109404: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-08 05:27:49.109411: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2024-05-08 05:27:49.109451: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-05-08 05:27:49.109481: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5564 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "\n",
    "import keras.datasets.mnist\n",
    "\n",
    "print(\"Python: \", sys.version_info)\n",
    "assert sys.version_info >= (3, 7)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "print(\"sklearn version: \", sklearn.__version__)\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "try:\n",
    "    # %tensorflow_version only exists in Colab.\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    # %tensorflow_version only exists in Colab.\n",
    "    %tensorflow_version 2.x\n",
    "    IS_COLAB = True\n",
    "except Exception:\n",
    "    IS_COLAB = False\n",
    "\n",
    "# TensorFlow ≥2.8 is required\n",
    "import tensorflow as tf\n",
    "print(\"TF version: \", tf.__version__)\n",
    "# assert tf.__version__ >= \"2.8\"\n",
    "\n",
    "if not tf.config.list_physical_devices('GPU'):\n",
    "    print(\"No GPU was detected. CNNs can be very slow without a GPU.\")\n",
    "    if IS_COLAB:\n",
    "        print(\"Go to Runtime > Change runtime and select a GPU hardware accelerator.\")\n",
    "\n",
    "# GPU test\n",
    "print(\"GPU installed: \",tf.test.is_built_with_gpu_support())\n",
    "\n",
    "# To prevent \"CUDNN_STATUS_ALLOC_FAILED\" error with GPUs\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow_datasets.core.utils import gcs_utils\n",
    "gcs_utils._is_gcs_disabled = True\n",
    "\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"ann\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)\n",
    "\n",
    "# Ignore useless warnings (see SciPy issue #5998)\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "IMAGES_PATH = Path() / \"images\" / \"deep\"\n",
    "IMAGES_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = IMAGES_PATH / f\"{fig_id}.{fig_extension}\"\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    savefig = plt.savefig(path, format=fig_extension, dpi=resolution)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-07T20:28:05.083426300Z",
     "start_time": "2024-05-07T20:28:05.023428900Z"
    }
   },
   "id": "b84aa6dfab0753c4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## get the data(EMNIST)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1864b631594ebb45"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "\n",
    "emnist = tfds.builder(\"emnist\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-07T20:41:45.225005500Z",
     "start_time": "2024-05-07T20:41:45.199591Z"
    }
   },
   "id": "6310befe51c3d605"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to /home/choi-wsl/tensorflow_datasets/emnist/byclass/3.0.0...\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": "Dl Completed...: 0 url [00:00, ? url/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8e77999f5fa74b5ea9b07773ad38243e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Dl Size...: 0 MiB [00:00, ? MiB/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4083b8cf14524227a31f626285fa569d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Extraction completed...: 0 file [00:00, ? file/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9c9b162e32cb442085d8648bbd934041"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NonMatchingChecksumError",
     "evalue": "Artifact https://www.itl.nist.gov/iaui/vip/cs_links/EMNIST/gzip.zip, downloaded to /home/choi-wsl/tensorflow_datasets/downloads/itl.nist.gov_iaui_vip_cs_links_EMNIST_gzipi4VnNviDSrfd9Zju6qv40flc3wr22t8ldulNStS6tmk.zip.tmp.40237038e9f34bcabc893fb6bb7b476f/itl, has wrong checksum:\n* Expected: UrlInfo(size=535.73 MiB, checksum='fb9bb67e33772a9cc0b895e4ecf36d2cf35be8b709693c3564cea2a019fcda8e', filename='gzip.zip')\n* Got: UrlInfo(size=110.07 KiB, checksum='505c7301158cdc1eb42fb8d64d16a341aa5a11d0513fba3f459eda7377d6644a', filename='itl')\nTo debug, see: https://www.tensorflow.org/datasets/overview#fixing_nonmatchingchecksumerror",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNonMatchingChecksumError\u001B[0m                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43memnist\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdownload_and_prepare\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow_datasets/core/logging/__init__.py:168\u001B[0m, in \u001B[0;36m_FunctionDecorator.__call__\u001B[0;34m(self, function, instance, args, kwargs)\u001B[0m\n\u001B[1;32m    166\u001B[0m metadata \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_start_call()\n\u001B[1;32m    167\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 168\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunction\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    169\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[1;32m    170\u001B[0m   metadata\u001B[38;5;241m.\u001B[39mmark_error()\n",
      "File \u001B[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow_datasets/core/dataset_builder.py:691\u001B[0m, in \u001B[0;36mDatasetBuilder.download_and_prepare\u001B[0;34m(self, download_dir, download_config, file_format)\u001B[0m\n\u001B[1;32m    689\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minfo\u001B[38;5;241m.\u001B[39mread_from_directory(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata_dir)\n\u001B[1;32m    690\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 691\u001B[0m   \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_download_and_prepare\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    692\u001B[0m \u001B[43m      \u001B[49m\u001B[43mdl_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdl_manager\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    693\u001B[0m \u001B[43m      \u001B[49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    694\u001B[0m \u001B[43m  \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    696\u001B[0m   \u001B[38;5;66;03m# NOTE: If modifying the lines below to put additional information in\u001B[39;00m\n\u001B[1;32m    697\u001B[0m   \u001B[38;5;66;03m# DatasetInfo, you'll likely also want to update\u001B[39;00m\n\u001B[1;32m    698\u001B[0m   \u001B[38;5;66;03m# DatasetInfo.read_from_directory to possibly restore these attributes\u001B[39;00m\n\u001B[1;32m    699\u001B[0m   \u001B[38;5;66;03m# when reading from package data.\u001B[39;00m\n\u001B[1;32m    700\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minfo\u001B[38;5;241m.\u001B[39mdownload_size \u001B[38;5;241m=\u001B[39m dl_manager\u001B[38;5;241m.\u001B[39mdownloaded_size\n",
      "File \u001B[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow_datasets/core/dataset_builder.py:1547\u001B[0m, in \u001B[0;36mGeneratorBasedBuilder._download_and_prepare\u001B[0;34m(self, dl_manager, download_config)\u001B[0m\n\u001B[1;32m   1545\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1546\u001B[0m   optional_pipeline_kwargs \u001B[38;5;241m=\u001B[39m {}\n\u001B[0;32m-> 1547\u001B[0m split_generators \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_split_generators\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# pylint: disable=unexpected-keyword-arg\u001B[39;49;00m\n\u001B[1;32m   1548\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdl_manager\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43moptional_pipeline_kwargs\u001B[49m\n\u001B[1;32m   1549\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1550\u001B[0m \u001B[38;5;66;03m# TODO(tfds): Could be removed once all datasets are migrated.\u001B[39;00m\n\u001B[1;32m   1551\u001B[0m \u001B[38;5;66;03m# https://github.com/tensorflow/datasets/issues/2537\u001B[39;00m\n\u001B[1;32m   1552\u001B[0m \u001B[38;5;66;03m# Legacy mode (eventually convert list[SplitGeneratorLegacy] -> dict)\u001B[39;00m\n\u001B[1;32m   1553\u001B[0m split_generators \u001B[38;5;241m=\u001B[39m split_builder\u001B[38;5;241m.\u001B[39mnormalize_legacy_split_generators(\n\u001B[1;32m   1554\u001B[0m     split_generators\u001B[38;5;241m=\u001B[39msplit_generators,\n\u001B[1;32m   1555\u001B[0m     generator_fn\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate_examples,\n\u001B[1;32m   1556\u001B[0m     is_beam\u001B[38;5;241m=\u001B[39m\u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m, BeamBasedBuilder),\n\u001B[1;32m   1557\u001B[0m )\n",
      "File \u001B[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow_datasets/image_classification/mnist.py:353\u001B[0m, in \u001B[0;36mEMNIST._split_generators\u001B[0;34m(self, dl_manager)\u001B[0m\n\u001B[1;32m    337\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_split_generators\u001B[39m(\u001B[38;5;28mself\u001B[39m, dl_manager):\n\u001B[1;32m    338\u001B[0m   filenames \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    339\u001B[0m       \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain_data\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124memnist-\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m-train-images-idx3-ubyte.gz\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m    340\u001B[0m           \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuilder_config\u001B[38;5;241m.\u001B[39mname\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    350\u001B[0m       ),\n\u001B[1;32m    351\u001B[0m   }\n\u001B[0;32m--> 353\u001B[0m   dir_name \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(\u001B[43mdl_manager\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdownload_and_extract\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mURL\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgzip\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    354\u001B[0m   extracted \u001B[38;5;241m=\u001B[39m dl_manager\u001B[38;5;241m.\u001B[39mextract(\n\u001B[1;32m    355\u001B[0m       {k: os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(dir_name, fname) \u001B[38;5;28;01mfor\u001B[39;00m k, fname \u001B[38;5;129;01min\u001B[39;00m filenames\u001B[38;5;241m.\u001B[39mitems()}\n\u001B[1;32m    356\u001B[0m   )\n\u001B[1;32m    358\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m [\n\u001B[1;32m    359\u001B[0m       tfds\u001B[38;5;241m.\u001B[39mcore\u001B[38;5;241m.\u001B[39mSplitGenerator(\n\u001B[1;32m    360\u001B[0m           name\u001B[38;5;241m=\u001B[39mtfds\u001B[38;5;241m.\u001B[39mSplit\u001B[38;5;241m.\u001B[39mTRAIN,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    374\u001B[0m       ),\n\u001B[1;32m    375\u001B[0m   ]\n",
      "File \u001B[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow_datasets/core/download/download_manager.py:688\u001B[0m, in \u001B[0;36mDownloadManager.download_and_extract\u001B[0;34m(self, url_or_urls)\u001B[0m\n\u001B[1;32m    686\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_downloader\u001B[38;5;241m.\u001B[39mtqdm():\n\u001B[1;32m    687\u001B[0m   \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_extractor\u001B[38;5;241m.\u001B[39mtqdm():\n\u001B[0;32m--> 688\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_map_promise\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_download_extract\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43murl_or_urls\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow_datasets/core/download/download_manager.py:831\u001B[0m, in \u001B[0;36m_map_promise\u001B[0;34m(map_fn, all_inputs)\u001B[0m\n\u001B[1;32m    827\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Map the function into each element and resolve the promise.\"\"\"\u001B[39;00m\n\u001B[1;32m    828\u001B[0m all_promises \u001B[38;5;241m=\u001B[39m tree_utils\u001B[38;5;241m.\u001B[39mmap_structure(\n\u001B[1;32m    829\u001B[0m     map_fn, all_inputs\n\u001B[1;32m    830\u001B[0m )  \u001B[38;5;66;03m# Apply the function\u001B[39;00m\n\u001B[0;32m--> 831\u001B[0m res \u001B[38;5;241m=\u001B[39m \u001B[43mtree_utils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap_structure\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    832\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mp\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mall_promises\u001B[49m\n\u001B[1;32m    833\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Wait promises\u001B[39;00m\n\u001B[1;32m    834\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
      "File \u001B[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tree/__init__.py:435\u001B[0m, in \u001B[0;36mmap_structure\u001B[0;34m(func, *structures, **kwargs)\u001B[0m\n\u001B[1;32m    432\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m other \u001B[38;5;129;01min\u001B[39;00m structures[\u001B[38;5;241m1\u001B[39m:]:\n\u001B[1;32m    433\u001B[0m   assert_same_structure(structures[\u001B[38;5;241m0\u001B[39m], other, check_types\u001B[38;5;241m=\u001B[39mcheck_types)\n\u001B[1;32m    434\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m unflatten_as(structures[\u001B[38;5;241m0\u001B[39m],\n\u001B[0;32m--> 435\u001B[0m                     [func(\u001B[38;5;241m*\u001B[39margs) \u001B[38;5;28;01mfor\u001B[39;00m args \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mmap\u001B[39m(flatten, structures))])\n",
      "File \u001B[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tree/__init__.py:435\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    432\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m other \u001B[38;5;129;01min\u001B[39;00m structures[\u001B[38;5;241m1\u001B[39m:]:\n\u001B[1;32m    433\u001B[0m   assert_same_structure(structures[\u001B[38;5;241m0\u001B[39m], other, check_types\u001B[38;5;241m=\u001B[39mcheck_types)\n\u001B[1;32m    434\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m unflatten_as(structures[\u001B[38;5;241m0\u001B[39m],\n\u001B[0;32m--> 435\u001B[0m                     [\u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m args \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mmap\u001B[39m(flatten, structures))])\n",
      "File \u001B[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow_datasets/core/download/download_manager.py:832\u001B[0m, in \u001B[0;36m_map_promise.<locals>.<lambda>\u001B[0;34m(p)\u001B[0m\n\u001B[1;32m    827\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Map the function into each element and resolve the promise.\"\"\"\u001B[39;00m\n\u001B[1;32m    828\u001B[0m all_promises \u001B[38;5;241m=\u001B[39m tree_utils\u001B[38;5;241m.\u001B[39mmap_structure(\n\u001B[1;32m    829\u001B[0m     map_fn, all_inputs\n\u001B[1;32m    830\u001B[0m )  \u001B[38;5;66;03m# Apply the function\u001B[39;00m\n\u001B[1;32m    831\u001B[0m res \u001B[38;5;241m=\u001B[39m tree_utils\u001B[38;5;241m.\u001B[39mmap_structure(\n\u001B[0;32m--> 832\u001B[0m     \u001B[38;5;28;01mlambda\u001B[39;00m p: \u001B[43mp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m, all_promises\n\u001B[1;32m    833\u001B[0m )  \u001B[38;5;66;03m# Wait promises\u001B[39;00m\n\u001B[1;32m    834\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
      "File \u001B[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/promise/promise.py:512\u001B[0m, in \u001B[0;36mPromise.get\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    510\u001B[0m target \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_target()\n\u001B[1;32m    511\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_wait(timeout \u001B[38;5;129;01mor\u001B[39;00m DEFAULT_TIMEOUT)\n\u001B[0;32m--> 512\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_target_settled_value\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_raise\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/promise/promise.py:516\u001B[0m, in \u001B[0;36mPromise._target_settled_value\u001B[0;34m(self, _raise)\u001B[0m\n\u001B[1;32m    514\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_target_settled_value\u001B[39m(\u001B[38;5;28mself\u001B[39m, _raise\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[1;32m    515\u001B[0m     \u001B[38;5;66;03m# type: (bool) -> Any\u001B[39;00m\n\u001B[0;32m--> 516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_target\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_settled_value\u001B[49m\u001B[43m(\u001B[49m\u001B[43m_raise\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/promise/promise.py:226\u001B[0m, in \u001B[0;36mPromise._settled_value\u001B[0;34m(self, _raise)\u001B[0m\n\u001B[1;32m    224\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _raise:\n\u001B[1;32m    225\u001B[0m     raise_val \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fulfillment_handler0\n\u001B[0;32m--> 226\u001B[0m     \u001B[43mreraise\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mtype\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mraise_val\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mraise_val\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_traceback\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    227\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fulfillment_handler0\n",
      "File \u001B[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/six.py:719\u001B[0m, in \u001B[0;36mreraise\u001B[0;34m(tp, value, tb)\u001B[0m\n\u001B[1;32m    717\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m value\u001B[38;5;241m.\u001B[39m__traceback__ \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m tb:\n\u001B[1;32m    718\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m value\u001B[38;5;241m.\u001B[39mwith_traceback(tb)\n\u001B[0;32m--> 719\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m value\n\u001B[1;32m    720\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    721\u001B[0m     value \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/promise/promise.py:87\u001B[0m, in \u001B[0;36mtry_catch\u001B[0;34m(handler, *args, **kwargs)\u001B[0m\n\u001B[1;32m     84\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtry_catch\u001B[39m(handler, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m     85\u001B[0m     \u001B[38;5;66;03m# type: (Callable, Any, Any) -> Union[Tuple[Any, None], Tuple[None, Tuple[Exception, Optional[TracebackType]]]]\u001B[39;00m\n\u001B[1;32m     86\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 87\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m (\u001B[43mhandler\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m     88\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     89\u001B[0m         tb \u001B[38;5;241m=\u001B[39m exc_info()[\u001B[38;5;241m2\u001B[39m]\n",
      "File \u001B[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow_datasets/core/download/download_manager.py:408\u001B[0m, in \u001B[0;36mDownloadManager._download.<locals>.<lambda>\u001B[0;34m(dl_result)\u001B[0m\n\u001B[1;32m    402\u001B[0m   future \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_downloader\u001B[38;5;241m.\u001B[39mdownload(\n\u001B[1;32m    403\u001B[0m       url, download_tmp_dir, verify\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_verify_ssl\n\u001B[1;32m    404\u001B[0m   )\n\u001B[1;32m    406\u001B[0m \u001B[38;5;66;03m# Post-process the result\u001B[39;00m\n\u001B[1;32m    407\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m future\u001B[38;5;241m.\u001B[39mthen(\n\u001B[0;32m--> 408\u001B[0m     \u001B[38;5;28;01mlambda\u001B[39;00m dl_result: \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_register_or_validate_checksums\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# pylint: disable=g-long-lambda\u001B[39;49;00m\n\u001B[1;32m    409\u001B[0m \u001B[43m        \u001B[49m\u001B[43murl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    410\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdl_result\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    411\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcomputed_url_info\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdl_result\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43murl_info\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    412\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexpected_url_info\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mexpected_url_info\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    413\u001B[0m \u001B[43m        \u001B[49m\u001B[43mchecksum_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchecksum_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    414\u001B[0m \u001B[43m        \u001B[49m\u001B[43murl_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43murl_path\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    415\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    416\u001B[0m )\n",
      "File \u001B[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow_datasets/core/download/download_manager.py:465\u001B[0m, in \u001B[0;36mDownloadManager._register_or_validate_checksums\u001B[0;34m(self, path, url, expected_url_info, computed_url_info, checksum_path, url_path)\u001B[0m\n\u001B[1;32m    454\u001B[0m   checksum_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_dl_path(url, computed_url_info\u001B[38;5;241m.\u001B[39mchecksum)\n\u001B[1;32m    455\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    456\u001B[0m   \u001B[38;5;66;03m# Eventually validate checksums\u001B[39;00m\n\u001B[1;32m    457\u001B[0m   \u001B[38;5;66;03m# Note:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    463\u001B[0m   \u001B[38;5;66;03m#   download). This is expected as it might mean the downloaded file\u001B[39;00m\n\u001B[1;32m    464\u001B[0m   \u001B[38;5;66;03m#   was corrupted. Note: The tmp file isn't deleted to allow inspection.\u001B[39;00m\n\u001B[0;32m--> 465\u001B[0m   \u001B[43m_validate_checksums\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    466\u001B[0m \u001B[43m      \u001B[49m\u001B[43murl\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43murl\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    467\u001B[0m \u001B[43m      \u001B[49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpath\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    468\u001B[0m \u001B[43m      \u001B[49m\u001B[43mexpected_url_info\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mexpected_url_info\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    469\u001B[0m \u001B[43m      \u001B[49m\u001B[43mcomputed_url_info\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcomputed_url_info\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    470\u001B[0m \u001B[43m      \u001B[49m\u001B[43mforce_checksums_validation\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_force_checksums_validation\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    471\u001B[0m \u001B[43m  \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    473\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_rename_and_get_final_dl_path(\n\u001B[1;32m    474\u001B[0m     url\u001B[38;5;241m=\u001B[39murl,\n\u001B[1;32m    475\u001B[0m     path\u001B[38;5;241m=\u001B[39mpath,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    479\u001B[0m     url_path\u001B[38;5;241m=\u001B[39murl_path,\n\u001B[1;32m    480\u001B[0m )\n",
      "File \u001B[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow_datasets/core/download/download_manager.py:809\u001B[0m, in \u001B[0;36m_validate_checksums\u001B[0;34m(url, path, computed_url_info, expected_url_info, force_checksums_validation)\u001B[0m\n\u001B[1;32m    797\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m    798\u001B[0m     expected_url_info\n\u001B[1;32m    799\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m computed_url_info\n\u001B[1;32m    800\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m expected_url_info \u001B[38;5;241m!=\u001B[39m computed_url_info\n\u001B[1;32m    801\u001B[0m ):\n\u001B[1;32m    802\u001B[0m   msg \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    803\u001B[0m       \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mArtifact \u001B[39m\u001B[38;5;132;01m{\u001B[39;00murl\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, downloaded to \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpath\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, has wrong checksum:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    804\u001B[0m       \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m* Expected: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mexpected_url_info\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    807\u001B[0m       \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mhttps://www.tensorflow.org/datasets/overview#fixing_nonmatchingchecksumerror\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m    808\u001B[0m   )\n\u001B[0;32m--> 809\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m NonMatchingChecksumError(msg)\n",
      "\u001B[0;31mNonMatchingChecksumError\u001B[0m: Artifact https://www.itl.nist.gov/iaui/vip/cs_links/EMNIST/gzip.zip, downloaded to /home/choi-wsl/tensorflow_datasets/downloads/itl.nist.gov_iaui_vip_cs_links_EMNIST_gzipi4VnNviDSrfd9Zju6qv40flc3wr22t8ldulNStS6tmk.zip.tmp.40237038e9f34bcabc893fb6bb7b476f/itl, has wrong checksum:\n* Expected: UrlInfo(size=535.73 MiB, checksum='fb9bb67e33772a9cc0b895e4ecf36d2cf35be8b709693c3564cea2a019fcda8e', filename='gzip.zip')\n* Got: UrlInfo(size=110.07 KiB, checksum='505c7301158cdc1eb42fb8d64d16a341aa5a11d0513fba3f459eda7377d6644a', filename='itl')\nTo debug, see: https://www.tensorflow.org/datasets/overview#fixing_nonmatchingchecksumerror"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    emnist.download_and_prepare()\n",
    "except:\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-07T20:41:57.392528800Z",
     "start_time": "2024-05-07T20:41:54.068541200Z"
    }
   },
   "id": "d845daf946db78b9"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasets/emnist\n"
     ]
    }
   ],
   "source": [
    "dpath = Path() / \"datasets\" / \"emnist\"\n",
    "print(dpath)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-07T21:03:36.388304400Z",
     "start_time": "2024-05-07T21:03:36.326306500Z"
    }
   },
   "id": "547bbe9dab2adb05"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Dataset emnist: could not find data in /home/choi-wsl/tensorflow_datasets. Please make sure to call dataset_builder.download_and_prepare(), or pass download=True to tfds.load() before trying to access the tf.data.Dataset object.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[21], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m test \u001B[38;5;241m=\u001B[39m \u001B[43mtfds\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43memnist\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdownload\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow_datasets/core/logging/__init__.py:168\u001B[0m, in \u001B[0;36m_FunctionDecorator.__call__\u001B[0;34m(self, function, instance, args, kwargs)\u001B[0m\n\u001B[1;32m    166\u001B[0m metadata \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_start_call()\n\u001B[1;32m    167\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 168\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunction\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    169\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[1;32m    170\u001B[0m   metadata\u001B[38;5;241m.\u001B[39mmark_error()\n",
      "File \u001B[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow_datasets/core/load.py:654\u001B[0m, in \u001B[0;36mload\u001B[0;34m(name, split, data_dir, batch_size, shuffle_files, download, as_supervised, decoders, read_config, with_info, builder_kwargs, download_and_prepare_kwargs, as_dataset_kwargs, try_gcs)\u001B[0m\n\u001B[1;32m    651\u001B[0m as_dataset_kwargs\u001B[38;5;241m.\u001B[39msetdefault(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mshuffle_files\u001B[39m\u001B[38;5;124m'\u001B[39m, shuffle_files)\n\u001B[1;32m    652\u001B[0m as_dataset_kwargs\u001B[38;5;241m.\u001B[39msetdefault(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mread_config\u001B[39m\u001B[38;5;124m'\u001B[39m, read_config)\n\u001B[0;32m--> 654\u001B[0m ds \u001B[38;5;241m=\u001B[39m \u001B[43mdbuilder\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mas_dataset\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mas_dataset_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    655\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m with_info:\n\u001B[1;32m    656\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m ds, dbuilder\u001B[38;5;241m.\u001B[39minfo\n",
      "File \u001B[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow_datasets/core/logging/__init__.py:168\u001B[0m, in \u001B[0;36m_FunctionDecorator.__call__\u001B[0;34m(self, function, instance, args, kwargs)\u001B[0m\n\u001B[1;32m    166\u001B[0m metadata \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_start_call()\n\u001B[1;32m    167\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 168\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunction\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    169\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m:\n\u001B[1;32m    170\u001B[0m   metadata\u001B[38;5;241m.\u001B[39mmark_error()\n",
      "File \u001B[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow_datasets/core/dataset_builder.py:864\u001B[0m, in \u001B[0;36mDatasetBuilder.as_dataset\u001B[0;34m(self, split, batch_size, shuffle_files, decoders, read_config, as_supervised)\u001B[0m\n\u001B[1;32m    862\u001B[0m \u001B[38;5;66;03m# pylint: enable=line-too-long\u001B[39;00m\n\u001B[1;32m    863\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata_path\u001B[38;5;241m.\u001B[39mexists():\n\u001B[0;32m--> 864\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAssertionError\u001B[39;00m(\n\u001B[1;32m    865\u001B[0m       \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataset \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m: could not find data in \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m. Please make sure to call \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    866\u001B[0m       \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdataset_builder.download_and_prepare(), or pass download=True to \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    867\u001B[0m       \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtfds.load() before trying to access the tf.data.Dataset object.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    868\u001B[0m       \u001B[38;5;241m%\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mname, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata_dir_root)\n\u001B[1;32m    869\u001B[0m   )\n\u001B[1;32m    871\u001B[0m \u001B[38;5;66;03m# By default, return all splits\u001B[39;00m\n\u001B[1;32m    872\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m split \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[0;31mAssertionError\u001B[0m: Dataset emnist: could not find data in /home/choi-wsl/tensorflow_datasets. Please make sure to call dataset_builder.download_and_prepare(), or pass download=True to tfds.load() before trying to access the tf.data.Dataset object."
     ]
    }
   ],
   "source": [
    "test = tfds.load(name=\"emnist\", download=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-07T21:55:20.166027900Z",
     "start_time": "2024-05-07T21:55:20.135339700Z"
    }
   },
   "id": "a567657d09d0a756"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "ddpath = Path() / \"datasets\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-07T21:28:34.619553400Z",
     "start_time": "2024-05-07T21:28:34.609553900Z"
    }
   },
   "id": "c1666584c9bd4bf4"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "class EMNIST2(tfds.image_classification.EMNIST):\n",
    "    URL = \"https://biometrics.nist.gov/cs_links/EMNIST/gzip.zip\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-07T22:19:45.183784900Z",
     "start_time": "2024-05-07T22:19:45.178785700Z"
    }
   },
   "id": "b69b8cbe7a53e5ff"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1mDownloading and preparing dataset Unknown size (download: Unknown size, generated: Unknown size, total: Unknown size) to /home/choi-wsl/tensorflow_datasets/emnis_t2/byclass/3.0.0...\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": "Dl Completed...: 0 url [00:00, ? url/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bc12f18da563462090bf27608c6c7ecf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Dl Size...: 0 MiB [00:00, ? MiB/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a136b4f1089846e9b055228e7cbee24a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Extraction completed...: 0 file [00:00, ? file/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d6b06b4497e743fca691ebb9791c2d46"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Extraction completed...: 0 file [00:00, ? file/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a40fc3f9b2a14608a6afc22a78948218"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating splits...:   0%|          | 0/2 [00:00<?, ? splits/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a42b5eea39614b8d9366c8bad7cc913c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating train examples...: 0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e31da4030a22443294abadf0c1fe7a81"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Shuffling /home/choi-wsl/tensorflow_datasets/emnis_t2/byclass/3.0.0.incompleteJNS6OJ/emnis_t2-train.tfrecord*.…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d47815a922db47c99e7380caa6ed2051"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating test examples...: 0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d3579185618d44f6b9a715db4e2f307f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Shuffling /home/choi-wsl/tensorflow_datasets/emnis_t2/byclass/3.0.0.incompleteJNS6OJ/emnis_t2-test.tfrecord*..…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "25e269694074412b8c607735418ccfe7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1mDataset emnis_t2 downloaded and prepared to /home/choi-wsl/tensorflow_datasets/emnis_t2/byclass/3.0.0. Subsequent calls will reuse this data.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "test = EMNIST2()\n",
    "\n",
    "test.download_and_prepare()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-07T22:28:02.037634500Z",
     "start_time": "2024-05-07T22:19:46.054702500Z"
    }
   },
   "id": "4cf2d91c06c50efb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "da59cc44506c3914"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
